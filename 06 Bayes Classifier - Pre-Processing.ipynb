{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What's up?\n"
     ]
    }
   ],
   "source": [
    "print('What\\'s up?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Notebook Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import walk\n",
    "from os.path import join\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from wordcloud import WordCloud\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXAMPLE_FILE = 'SpamData/01_Processing/practice_email.txt'\n",
    "\n",
    "SPAM_1_PATH = 'SpamData/01_Processing/spam_assassin_corpus/spam_1'\n",
    "SPAM_2_PATH = 'SpamData/01_Processing/spam_assassin_corpus/spam_2'\n",
    "EASY_NONSPAM_1_PATH = 'SpamData/01_Processing/spam_assassin_corpus/easy_ham_1'\n",
    "EASY_NONSPAM_2_PATH = 'SpamData/01_Processing/spam_assassin_corpus/easy_ham_2'\n",
    "\n",
    "SPAM_CAT = 1\n",
    "HAM_CAT = 0\n",
    "VOCAB_SIZE = 2500\n",
    "\n",
    "DATA_JSON_FILE = 'SpamData/01_Processing/email-text-data.json'\n",
    "WORD_ID_FILE = 'SpamData/01_Processing/word-by-id.csv'\n",
    "\n",
    "TRAINING_DATA_FILE = 'SpamData/02_Training/train-data.txt'\n",
    "TEST_DATA_FILE = 'SpamData/02_Training/test-data.txt'\n",
    "\n",
    "WHALE_FILE = 'SpamData/01_Processing/wordcloud_resources/whale-icon.png'\n",
    "SKULL_FILE = 'SpamData/01_Processing/wordcloud_resources/skull-icon.png'\n",
    "THUMBS_UP_FILE = 'SpamData/01_Processing/wordcloud_resources/thumbs-up.png'\n",
    "THUMBS_DOWN_FILE = 'SpamData/01_Processing/wordcloud_resources/thumbs-down.png'\n",
    "CUSTOM_FONT_FILE = 'SpamData/01_Processing/wordcloud_resources/OpenSansCondensed-Bold.ttf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "From exmh-workers-admin@redhat.com  Thu Aug 22 12:36:23 2002\n",
      "Return-Path: <exmh-workers-admin@spamassassin.taint.org>\n",
      "Delivered-To: zzzz@localhost.netnoteinc.com\n",
      "Received: from localhost (localhost [127.0.0.1])\n",
      "\tby phobos.labs.netnoteinc.com (Postfix) with ESMTP id D03E543C36\n",
      "\tfor <zzzz@localhost>; Thu, 22 Aug 2002 07:36:16 -0400 (EDT)\n",
      "Received: from phobos [127.0.0.1]\n",
      "\tby localhost with IMAP (fetchmail-5.9.0)\n",
      "\tfor zzzz@localhost (single-drop); Thu, 22 Aug 2002 12:36:16 +0100 (IST)\n",
      "Received: from listman.spamassassin.taint.org (listman.spamassassin.taint.org [66.187.233.211]) by\n",
      "    dogma.slashnull.org (8.11.6/8.11.6) with ESMTP id g7MBYrZ04811 for\n",
      "    <zzzz-exmh@spamassassin.taint.org>; Thu, 22 Aug 2002 12:34:53 +0100\n",
      "Received: from listman.spamassassin.taint.org (localhost.localdomain [127.0.0.1]) by\n",
      "    listman.redhat.com (Postfix) with ESMTP id 8386540858; Thu, 22 Aug 2002\n",
      "    07:35:02 -0400 (EDT)\n",
      "Delivered-To: exmh-workers@listman.spamassassin.taint.org\n",
      "Received: from int-mx1.corp.spamassassin.taint.org (int-mx1.corp.spamassassin.taint.org\n",
      "    [172.16.52.254]) by listman.redhat.com (Postfix) with ESMTP id 10CF8406D7\n",
      "    for <exmh-workers@listman.redhat.com>; Thu, 22 Aug 2002 07:34:10 -0400\n",
      "    (EDT)\n",
      "Received: (from mail@localhost) by int-mx1.corp.spamassassin.taint.org (8.11.6/8.11.6)\n",
      "    id g7MBY7g11259 for exmh-workers@listman.redhat.com; Thu, 22 Aug 2002\n",
      "    07:34:07 -0400\n",
      "Received: from mx1.spamassassin.taint.org (mx1.spamassassin.taint.org [172.16.48.31]) by\n",
      "    int-mx1.corp.redhat.com (8.11.6/8.11.6) with SMTP id g7MBY7Y11255 for\n",
      "    <exmh-workers@redhat.com>; Thu, 22 Aug 2002 07:34:07 -0400\n",
      "Received: from ratree.psu.ac.th ([202.28.97.6]) by mx1.spamassassin.taint.org\n",
      "    (8.11.6/8.11.6) with SMTP id g7MBIhl25223 for <exmh-workers@redhat.com>;\n",
      "    Thu, 22 Aug 2002 07:18:55 -0400\n",
      "Received: from delta.cs.mu.OZ.AU (delta.coe.psu.ac.th [172.30.0.98]) by\n",
      "    ratree.psu.ac.th (8.11.6/8.11.6) with ESMTP id g7MBWel29762;\n",
      "    Thu, 22 Aug 2002 18:32:40 +0700 (ICT)\n",
      "Received: from munnari.OZ.AU (localhost [127.0.0.1]) by delta.cs.mu.OZ.AU\n",
      "    (8.11.6/8.11.6) with ESMTP id g7MBQPW13260; Thu, 22 Aug 2002 18:26:25\n",
      "    +0700 (ICT)\n",
      "From: Robert Elz <kre@munnari.OZ.AU>\n",
      "To: Chris Garrigues <cwg-dated-1030377287.06fa6d@DeepEddy.Com>\n",
      "Cc: exmh-workers@spamassassin.taint.org\n",
      "Subject: Re: New Sequences Window\n",
      "In-Reply-To: <1029945287.4797.TMDA@deepeddy.vircio.com>\n",
      "References: <1029945287.4797.TMDA@deepeddy.vircio.com>\n",
      "    <1029882468.3116.TMDA@deepeddy.vircio.com> <9627.1029933001@munnari.OZ.AU>\n",
      "    <1029943066.26919.TMDA@deepeddy.vircio.com>\n",
      "    <1029944441.398.TMDA@deepeddy.vircio.com>\n",
      "MIME-Version: 1.0\n",
      "Content-Type: text/plain; charset=us-ascii\n",
      "Message-Id: <13258.1030015585@munnari.OZ.AU>\n",
      "X-Loop: exmh-workers@spamassassin.taint.org\n",
      "Sender: exmh-workers-admin@spamassassin.taint.org\n",
      "Errors-To: exmh-workers-admin@spamassassin.taint.org\n",
      "X-Beenthere: exmh-workers@spamassassin.taint.org\n",
      "X-Mailman-Version: 2.0.1\n",
      "Precedence: bulk\n",
      "List-Help: <mailto:exmh-workers-request@spamassassin.taint.org?subject=help>\n",
      "List-Post: <mailto:exmh-workers@spamassassin.taint.org>\n",
      "List-Subscribe: <https://listman.spamassassin.taint.org/mailman/listinfo/exmh-workers>,\n",
      "    <mailto:exmh-workers-request@redhat.com?subject=subscribe>\n",
      "List-Id: Discussion list for EXMH developers <exmh-workers.spamassassin.taint.org>\n",
      "List-Unsubscribe: <https://listman.spamassassin.taint.org/mailman/listinfo/exmh-workers>,\n",
      "    <mailto:exmh-workers-request@redhat.com?subject=unsubscribe>\n",
      "List-Archive: <https://listman.spamassassin.taint.org/mailman/private/exmh-workers/>\n",
      "Date: Thu, 22 Aug 2002 18:26:25 +0700\n",
      "\n",
      "\n",
      "Dear Mr Still\n",
      "\n",
      "Good tidings to you and all your staff for the festive season ahead (Christmas).\n",
      "Now to the crux of the matter-in-hand: I am a fully qualified Santa Claus and am wondering whether you might consider me to run my own \"Santa's Grotto\" in your store.\n",
      "But WAIT! You're probably thinking: \"What makes him so special?\"\n",
      "Well, first of all, I have made several changes to the characterisation of Father Christmas. Rather than greeting the children with shouts of \"Ho, ho, ho!\" I prefer to whisper the phrase \"Dependence is not unfathomable in this cruel world we live in\". In addition, my gifts are ALL hand-made, ranging from felt hoops to vanilla-pod holders.\n",
      "You will note also, from the enclosed sketch, that I have radically redesigned Santa's outfit and have renamed my character \"Lord Buckles\". Would you be interested in employing me? I promise NEVER to let you down.\n",
      "I look forward to hearing from you.\n",
      "\n",
      "Best wishes\n",
      "Robin Cooper\n",
      "[Excerpt from the book: The Timewaster Letters by Robin Cooper]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stream = open(EXAMPLE_FILE, encoding='latin-1')\n",
    "message = stream.read()\n",
    "stream.close()\n",
    "\n",
    "print(type(message))\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'utf-8'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.getfilesystemencoding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Dear Mr Still\n",
      "\n",
      "\n",
      "\n",
      "Good tidings to you and all your staff for the festive season ahead (Christmas).\n",
      "\n",
      "Now to the crux of the matter-in-hand: I am a fully qualified Santa Claus and am wondering whether you might consider me to run my own \"Santa's Grotto\" in your store.\n",
      "\n",
      "But WAIT! You're probably thinking: \"What makes him so special?\"\n",
      "\n",
      "Well, first of all, I have made several changes to the characterisation of Father Christmas. Rather than greeting the children with shouts of \"Ho, ho, ho!\" I prefer to whisper the phrase \"Dependence is not unfathomable in this cruel world we live in\". In addition, my gifts are ALL hand-made, ranging from felt hoops to vanilla-pod holders.\n",
      "\n",
      "You will note also, from the enclosed sketch, that I have radically redesigned Santa's outfit and have renamed my character \"Lord Buckles\". Would you be interested in employing me? I promise NEVER to let you down.\n",
      "\n",
      "I look forward to hearing from you.\n",
      "\n",
      "\n",
      "\n",
      "Best wishes\n",
      "\n",
      "Robin Cooper\n",
      "\n",
      "[Excerpt from the book: The Timewaster Letters by Robin Cooper]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stream = open(EXAMPLE_FILE, encoding='latin-1')\n",
    "\n",
    "is_body = False\n",
    "lines = []\n",
    "\n",
    "for line in stream:\n",
    "    if is_body:\n",
    "        lines.append(line)\n",
    "    elif line == '\\n':\n",
    "        is_body = True\n",
    "\n",
    "stream.close()\n",
    "\n",
    "email_body = '\\n'.join(lines)\n",
    "print(email_body)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Generator Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_squares(N):\n",
    "    for my_number in range(N):\n",
    "        yield my_number ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ->1 ->4 ->9 ->16 ->"
     ]
    }
   ],
   "source": [
    "for i in generate_squares(5):\n",
    "    print(i, end=' ->')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Email body extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def email_body_generator(path):\n",
    "    \n",
    "    for root, dirnames, filenames in walk(path):\n",
    "        for file_name in filenames:\n",
    "            \n",
    "            filepath = join(root, file_name)\n",
    "            \n",
    "            stream = open(filepath, encoding='latin-1')\n",
    "\n",
    "            is_body = False\n",
    "            lines = []\n",
    "\n",
    "            for line in stream:\n",
    "                if is_body:\n",
    "                    lines.append(line)\n",
    "                elif line == '\\n':\n",
    "                    is_body = True\n",
    "\n",
    "            stream.close()\n",
    "\n",
    "            email_body = '\\n'.join(lines)\n",
    "            \n",
    "            yield file_name, email_body\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_from_directory(path, classification):\n",
    "    rows = []\n",
    "    row_names = []\n",
    "    \n",
    "    for file_name, email_body in email_body_generator(path):\n",
    "        rows.append({'MESSAGE': email_body, 'CATEGORY': classification})\n",
    "        row_names.append(file_name)\n",
    "        \n",
    "    return pd.DataFrame(rows, index=row_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MESSAGE</th>\n",
       "      <th>CATEGORY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>00001.7848dde101aa985090474a91ec93fcf0</th>\n",
       "      <td>&lt;!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.0 Tr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00002.d94f1b97e48ed3b553b3508d116e6a09</th>\n",
       "      <td>1) Fight The Risk of Cancer!\\n\\nhttp://www.adc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00003.2ee33bc6eacdb11f38d052c44819ba6c</th>\n",
       "      <td>1) Fight The Risk of Cancer!\\n\\nhttp://www.adc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00004.eac8de8d759b7e74154f142194282724</th>\n",
       "      <td>##############################################...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00005.57696a39d7d84318ce497886896bf90d</th>\n",
       "      <td>I thought you might like these:\\n\\n1) Slim Dow...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                  MESSAGE  \\\n",
       "00001.7848dde101aa985090474a91ec93fcf0  <!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.0 Tr...   \n",
       "00002.d94f1b97e48ed3b553b3508d116e6a09  1) Fight The Risk of Cancer!\\n\\nhttp://www.adc...   \n",
       "00003.2ee33bc6eacdb11f38d052c44819ba6c  1) Fight The Risk of Cancer!\\n\\nhttp://www.adc...   \n",
       "00004.eac8de8d759b7e74154f142194282724  ##############################################...   \n",
       "00005.57696a39d7d84318ce497886896bf90d  I thought you might like these:\\n\\n1) Slim Dow...   \n",
       "\n",
       "                                        CATEGORY  \n",
       "00001.7848dde101aa985090474a91ec93fcf0         1  \n",
       "00002.d94f1b97e48ed3b553b3508d116e6a09         1  \n",
       "00003.2ee33bc6eacdb11f38d052c44819ba6c         1  \n",
       "00004.eac8de8d759b7e74154f142194282724         1  \n",
       "00005.57696a39d7d84318ce497886896bf90d         1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_emails = df_from_directory(SPAM_1_PATH, 1)\n",
    "spam_emails = spam_emails.append(df_from_directory(SPAM_2_PATH, 1))\n",
    "spam_emails.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1898, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_emails.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3901, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ham_emails = df_from_directory(EASY_NONSPAM_1_PATH, HAM_CAT)\n",
    "ham_emails = ham_emails.append(df_from_directory(EASY_NONSPAM_2_PATH, HAM_CAT))\n",
    "ham_emails.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of entire dataframe is  (5799, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MESSAGE</th>\n",
       "      <th>CATEGORY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>00001.7848dde101aa985090474a91ec93fcf0</th>\n",
       "      <td>&lt;!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.0 Tr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00002.d94f1b97e48ed3b553b3508d116e6a09</th>\n",
       "      <td>1) Fight The Risk of Cancer!\\n\\nhttp://www.adc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00003.2ee33bc6eacdb11f38d052c44819ba6c</th>\n",
       "      <td>1) Fight The Risk of Cancer!\\n\\nhttp://www.adc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00004.eac8de8d759b7e74154f142194282724</th>\n",
       "      <td>##############################################...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00005.57696a39d7d84318ce497886896bf90d</th>\n",
       "      <td>I thought you might like these:\\n\\n1) Slim Dow...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                  MESSAGE  \\\n",
       "00001.7848dde101aa985090474a91ec93fcf0  <!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.0 Tr...   \n",
       "00002.d94f1b97e48ed3b553b3508d116e6a09  1) Fight The Risk of Cancer!\\n\\nhttp://www.adc...   \n",
       "00003.2ee33bc6eacdb11f38d052c44819ba6c  1) Fight The Risk of Cancer!\\n\\nhttp://www.adc...   \n",
       "00004.eac8de8d759b7e74154f142194282724  ##############################################...   \n",
       "00005.57696a39d7d84318ce497886896bf90d  I thought you might like these:\\n\\n1) Slim Dow...   \n",
       "\n",
       "                                        CATEGORY  \n",
       "00001.7848dde101aa985090474a91ec93fcf0         1  \n",
       "00002.d94f1b97e48ed3b553b3508d116e6a09         1  \n",
       "00003.2ee33bc6eacdb11f38d052c44819ba6c         1  \n",
       "00004.eac8de8d759b7e74154f142194282724         1  \n",
       "00005.57696a39d7d84318ce497886896bf90d         1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.concat([spam_emails, ham_emails])\n",
    "print('Shape of entire dataframe is ', data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MESSAGE</th>\n",
       "      <th>CATEGORY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>01396.61983fbe6ec43f55fd44e30fce24ffa6</th>\n",
       "      <td>http://news.bbc.co.uk/1/hi/england/2515127.stm...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01397.9f9ef4c2a8dc012d80f2ce2d3473d3b7</th>\n",
       "      <td>&gt; &gt;-- be careful when using this one.) Also, t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01398.169b51731fe569f42169ae8f948ec676</th>\n",
       "      <td>&gt;&gt;&gt;&gt;&gt; \"SM\" == Skip Montanaro &lt;skip@pobox.com&gt; ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01399.ca6b00b7b341bbde9a9ea3dd6a7bf896</th>\n",
       "      <td>So then, \"Mark Hammond\" &lt;mhammond@skippinet.co...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01400.f897f0931e461e7b2e964d28e927c35e</th>\n",
       "      <td>Hi there,\\n\\n\\n\\nNow this is probably of no us...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                  MESSAGE  \\\n",
       "01396.61983fbe6ec43f55fd44e30fce24ffa6  http://news.bbc.co.uk/1/hi/england/2515127.stm...   \n",
       "01397.9f9ef4c2a8dc012d80f2ce2d3473d3b7  > >-- be careful when using this one.) Also, t...   \n",
       "01398.169b51731fe569f42169ae8f948ec676  >>>>> \"SM\" == Skip Montanaro <skip@pobox.com> ...   \n",
       "01399.ca6b00b7b341bbde9a9ea3dd6a7bf896  So then, \"Mark Hammond\" <mhammond@skippinet.co...   \n",
       "01400.f897f0931e461e7b2e964d28e927c35e  Hi there,\\n\\n\\n\\nNow this is probably of no us...   \n",
       "\n",
       "                                        CATEGORY  \n",
       "01396.61983fbe6ec43f55fd44e30fce24ffa6         0  \n",
       "01397.9f9ef4c2a8dc012d80f2ce2d3473d3b7         0  \n",
       "01398.169b51731fe569f42169ae8f948ec676         0  \n",
       "01399.ca6b00b7b341bbde9a9ea3dd6a7bf896         0  \n",
       "01400.f897f0931e461e7b2e964d28e927c35e         0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning: Checking for Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if any message bodies are null\n",
    "data['MESSAGE'].isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_var = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(my_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if there are empty emails (string length zero)\n",
    "(data.MESSAGE.str.len() == 0).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(data.MESSAGE.str.len() == 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Challenge: how would you check the number of entries with null/None values?\n",
    "data.MESSAGE.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locate empty emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data.MESSAGE.str.len() == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['cmds', 'cmds', 'cmds'], dtype='object')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data.MESSAGE.str.len() == 0].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'.DS_Store'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mE:\\Anaconda\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2645\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2646\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2647\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine._get_loc_duplicates\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine._maybe_get_bool_indexer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '.DS_Store'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-05d3e7949f3f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.DS_Store'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\Anaconda\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2646\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2647\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2648\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2649\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2650\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine._get_loc_duplicates\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine._maybe_get_bool_indexer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '.DS_Store'"
     ]
    }
   ],
   "source": [
    "data.index.get_loc('.DS_Store')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[4608:4611]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove System File Entries from Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['cmds', '.DS_Store'], inplace=True)\n",
    "\n",
    "data[4608:4611]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Document IDs to Track Emails in Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_ids = range(0, len(data.index))\n",
    "data['DOC_ID'] = document_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data['FILE_NAME'] = data.index\n",
    "data.set_index('DOC_ID', inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save to File using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_json(DATA_JSON_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of Spam Messages Visualised (Pie Charts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.CATEGORY.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amount_of_spam = data.CATEGORY.value_counts()[1]\n",
    "amount_of_ham = data.CATEGORY.value_counts()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_names = ['Spam', 'Legit Mail']\n",
    "sizes = [amount_of_spam, amount_of_ham]\n",
    "\n",
    "plt.figure(figsize=(2, 2), dpi=227)\n",
    "plt.pie(sizes, labels=category_names, textprops={'fontsize': 6}, startangle=90, \n",
    "       autopct='%1.0f%%')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_names = ['Spam', 'Legit Mail']\n",
    "sizes = [amount_of_spam, amount_of_ham]\n",
    "custom_colours = ['#ff7675', '#74b9ff']\n",
    "\n",
    "plt.figure(figsize=(2, 2), dpi=227)\n",
    "plt.pie(sizes, labels=category_names, textprops={'fontsize': 6}, startangle=90, \n",
    "       autopct='%1.0f%%', colors=custom_colours, explode=[0, 0.1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_names = ['Spam', 'Legit Mail']\n",
    "sizes = [amount_of_spam, amount_of_ham]\n",
    "custom_colours = ['#ff7675', '#74b9ff']\n",
    "\n",
    "plt.figure(figsize=(2, 2), dpi=227)\n",
    "plt.pie(sizes, labels=category_names, textprops={'fontsize': 6}, startangle=90, \n",
    "       autopct='%1.0f%%', colors=custom_colours, pctdistance=0.8)\n",
    "\n",
    "# draw circle\n",
    "centre_circle = plt.Circle((0, 0), radius=0.6, fc='white')\n",
    "plt.gca().add_artist(centre_circle)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_names = ['Spam', 'Legit Mail', 'Updates', 'Promotions']\n",
    "sizes = [25, 43, 19, 22]\n",
    "custom_colours = ['#ff7675', '#74b9ff', '#55efc4', '#ffeaa7']\n",
    "offset = [0.05, 0.05, 0.05, 0.05]\n",
    "\n",
    "plt.figure(figsize=(2, 2), dpi=227)\n",
    "plt.pie(sizes, labels=category_names, textprops={'fontsize': 6}, startangle=90, \n",
    "       autopct='%1.0f%%', colors=custom_colours, pctdistance=0.8, explode=offset)\n",
    "\n",
    "# draw circle\n",
    "centre_circle = plt.Circle((0, 0), radius=0.6, fc='white')\n",
    "plt.gca().add_artist(centre_circle)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to lower case\n",
    "msg = 'All work and no play makes Jack a dull boy.'\n",
    "msg.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the NLTK Resources (Tokenizer & Stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('gutenberg')\n",
    "nltk.download('shakespeare')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = 'All work and no play makes Jack a dull boy.'\n",
    "word_tokenize(msg.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Stop Words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'this' in stop_words: print('Found it!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge: print out 'Nope. Not in here' if the word \"hello\" is not contained in stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'hello' not in stop_words: print('Nope. Not in here')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = 'All work and no play makes Jack a dull boy. To be or not to be.'\n",
    "words = word_tokenize(msg.lower())\n",
    "\n",
    "filtered_words = []\n",
    "# Challenge: append non-stop words to filtered_words\n",
    "for word in words:\n",
    "    if word not in stop_words:\n",
    "        filtered_words.append(word)\n",
    "\n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Stems and Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "msg = 'All work and no play makes Jack a dull boy. To be or not to be. \\\n",
    "      Nobody expects the Spanish Inquisition!'\n",
    "words = word_tokenize(msg.lower())\n",
    "\n",
    "# stemmer = PorterStemmer()\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "\n",
    "filtered_words = []\n",
    "# Challenge: append non-stop words to filtered_words\n",
    "for word in words:\n",
    "    if word not in stop_words:\n",
    "        stemmed_word = stemmer.stem(word)\n",
    "        filtered_words.append(stemmed_word)\n",
    "\n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'p'.isalpha()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'?'.isalpha()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "msg = 'All work and no play makes Jack a dull boy. To be or not to be. ??? \\\n",
    "      Nobody expects the Spanish Inquisition!'\n",
    "\n",
    "words = word_tokenize(msg.lower())\n",
    "stemmer = SnowballStemmer('english')\n",
    "filtered_words = []\n",
    "\n",
    "for word in words:\n",
    "    if word not in stop_words and word.isalpha():\n",
    "        stemmed_word = stemmer.stem(word)\n",
    "        filtered_words.append(stemmed_word)\n",
    "\n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing HTML tags from Emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(data.at[2, 'MESSAGE'], 'html.parser')\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for Email Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_message(message, stemmer=PorterStemmer(), \n",
    "                 stop_words=set(stopwords.words('english'))):\n",
    "    \n",
    "    # Converts to Lower Case and splits up the words\n",
    "    words = word_tokenize(message.lower())\n",
    "    \n",
    "    filtered_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        # Removes the stop words and punctuation\n",
    "        if word not in stop_words and word.isalpha():\n",
    "            filtered_words.append(stemmer.stem(word))\n",
    "    \n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clean_message(email_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge: Modify function to remove HTML tags. Then test on Email with DOC_ID 2. \n",
    "def clean_msg_no_html(message, stemmer=PorterStemmer(), \n",
    "                 stop_words=set(stopwords.words('english'))):\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    soup = BeautifulSoup(message, 'html.parser')\n",
    "    cleaned_text = soup.get_text()\n",
    "    \n",
    "    # Converts to Lower Case and splits up the words\n",
    "    words = word_tokenize(cleaned_text.lower())\n",
    "    \n",
    "    filtered_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        # Removes the stop words and punctuation\n",
    "        if word not in stop_words and word.isalpha():\n",
    "            filtered_words.append(stemmer.stem(word))\n",
    "#             filtered_words.append(word) \n",
    "    \n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_msg_no_html(data.at[2, 'MESSAGE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Cleaning and Tokenisation to all messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slicing Dataframes and Series & Creating Subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iat[2, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[5:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_emails = data.MESSAGE.iloc[0:3]\n",
    "\n",
    "nested_list = first_emails.apply(clean_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flat_list = []\n",
    "# for sublist in nested_list:\n",
    "#     for item in sublist:\n",
    "#         flat_list.append(item)\n",
    "\n",
    "flat_list = [item for sublist in nested_list for item in sublist]\n",
    "        \n",
    "len(flat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# use apply() on all the messages in the dataframe\n",
    "nested_list = data.MESSAGE.apply(clean_msg_no_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_list.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Logic to Slice Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.CATEGORY == 1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.CATEGORY == 1].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge: create two variables (doc_ids_spam, doc_ids_ham) which \n",
    "# hold onto the indices for the spam and the non-spam emails respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ids_spam = data[data.CATEGORY == 1].index\n",
    "doc_ids_ham = data[data.CATEGORY == 0].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ids_ham"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsetting a Series with an Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(doc_ids_ham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(nested_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_list_ham = nested_list.loc[doc_ids_ham]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_list_ham.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_list_ham.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_list_spam = nested_list.loc[doc_ids_spam]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge: use python list comprehension and then find the total number of \n",
    "# words in our cleaned dataset of spam email bodies. Also find the total number of \n",
    "# words in normal emails in the dataset. Then find the 10 most common words used in \n",
    "# spam. Also, find the 10 most common words used in non-spam messages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list_ham = [item for sublist in nested_list_ham for item in sublist]\n",
    "normal_words = pd.Series(flat_list_ham).value_counts()\n",
    "\n",
    "normal_words.shape[0] # total number of unique words in the non-spam messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list_spam = [item for sublist in nested_list_spam for item in sublist]\n",
    "spammy_words = pd.Series(flat_list_spam).value_counts()\n",
    "\n",
    "spammy_words.shape[0] # total number of unique words in the spam messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spammy_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cloud = WordCloud().generate(email_body)\n",
    "plt.imshow(word_cloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_corpus = nltk.corpus.gutenberg.words('melville-moby_dick.txt')\n",
    "len(example_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(example_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = [''.join(word) for word in example_corpus]\n",
    "novel_as_string = ' '.join(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icon = Image.open(WHALE_FILE)\n",
    "image_mask = Image.new(mode='RGB', size=icon.size, color=(255, 255, 255))\n",
    "image_mask.paste(icon, box=icon)\n",
    "\n",
    "rgb_array = np.array(image_mask) # converts the image object to an array\n",
    "\n",
    "word_cloud = WordCloud(mask=rgb_array, background_color='white', \n",
    "                      max_words=400, colormap='ocean')\n",
    "\n",
    "word_cloud.generate(novel_as_string)\n",
    "\n",
    "plt.figure(figsize=[16, 8])\n",
    "plt.imshow(word_cloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_array[1023, 2047]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_array[500, 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge: use the skull image in the lesson resources to create a word cloud\n",
    "# for Shakespeare's play Hamlet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hamlet_corpus = nltk.corpus.gutenberg.words('shakespeare-hamlet.txt')\n",
    "word_list = [''.join(word) for word in hamlet_corpus]\n",
    "hamlet_as_string = ' '.join(word_list)\n",
    "\n",
    "skull_icon = Image.open(SKULL_FILE)\n",
    "image_mask = Image.new(mode='RGB', size=skull_icon.size, color=(255, 255, 255))\n",
    "image_mask.paste(skull_icon, box=skull_icon)\n",
    "rgb_array = np.array(image_mask)\n",
    "\n",
    "word_cloud = WordCloud(mask=rgb_array, background_color='white',\n",
    "                      colormap='bone', max_words=600)\n",
    "\n",
    "word_cloud.generate(hamlet_as_string)\n",
    "\n",
    "plt.figure(figsize=[16, 8])\n",
    "plt.imshow(word_cloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Cloud of Ham and Spam Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icon = Image.open(THUMBS_UP_FILE)\n",
    "image_mask = Image.new(mode='RGB', size=icon.size, color=(255, 255, 255))\n",
    "image_mask.paste(icon, box=icon)\n",
    "\n",
    "rgb_array = np.array(image_mask) # converts the image object to an array\n",
    "\n",
    "# Generate the text as a string for the word cloud\n",
    "ham_str = ' '.join(flat_list_ham)\n",
    "\n",
    "word_cloud = WordCloud(mask=rgb_array, background_color='white', \n",
    "                      max_words=500, colormap='winter')\n",
    "\n",
    "word_cloud.generate(ham_str)\n",
    "\n",
    "plt.figure(figsize=[16, 8])\n",
    "plt.imshow(word_cloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge: Look at the word cloud documentation. Use the custom font included in the \n",
    "# lesson resources instead of the default font and create a word cloud of the spammy words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icon = Image.open(THUMBS_DOWN_FILE)\n",
    "image_mask = Image.new(mode='RGB', size=icon.size, color=(255, 255, 255))\n",
    "image_mask.paste(icon, box=icon)\n",
    "\n",
    "rgb_array = np.array(image_mask) # converts the image object to an array\n",
    "\n",
    "# Generate the text as a string for the word cloud\n",
    "spam_str = ' '.join(flat_list_spam)\n",
    "\n",
    "word_cloud = WordCloud(mask=rgb_array, background_color='white', max_font_size=300,\n",
    "                      max_words=2000, colormap='gist_heat', font_path=CUSTOM_FONT_FILE)\n",
    "\n",
    "word_cloud.generate(spam_str.upper())\n",
    "\n",
    "plt.figure(figsize=[16, 8])\n",
    "plt.imshow(word_cloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Generate Vocabulary & Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_nested_list = data.MESSAGE.apply(clean_msg_no_html)\n",
    "flat_stemmed_list = [item for sublist in stemmed_nested_list for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = pd.Series(flat_stemmed_list).value_counts()\n",
    "print('Nr of unique words', unique_words.shape[0])\n",
    "unique_words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge: Create subset of the series called 'frequent_words' that only contains\n",
    "# the most common 2,500 words out of the total. Print out the top 10 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_words = unique_words[0:VOCAB_SIZE]\n",
    "print('Most common words: \\n', frequent_words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Vocabulary DataFrame with a WORD_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_ids = list(range(0, VOCAB_SIZE))\n",
    "vocab = pd.DataFrame({'VOCAB_WORD': frequent_words.index.values}, index=word_ids)\n",
    "vocab.index.name = 'WORD_ID'\n",
    "vocab.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Vocabulary as a CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.to_csv(WORD_ID_FILE, index_label=vocab.index.name, header=vocab.VOCAB_WORD.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Checking if a Word is Part of the Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge: Write a line of code that checks if a particular word is part \n",
    "# of the vocabulary. Your code should return True if the word is among the \n",
    "# 2,500 words that comprise the vocabulary, and False otherwise. Check these words:\n",
    "# 'machine'\n",
    "# 'learning'\n",
    "# 'fun'\n",
    "# 'learn'\n",
    "# 'data'\n",
    "# 'science'\n",
    "# 'app'\n",
    "# 'brewery'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "any(vocab.VOCAB_WORD == 'machine') # inefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'brew' in set(vocab.VOCAB_WORD) # better way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Find the Email with the Most Number of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge: Print out the number of words in the longest email (after cleaning & stemming).\n",
    "# Note the longest email's position in the list of cleaned emails. Print out the stemmed\n",
    "# list of words in the longest email. Print out the longest email from the data dataframe.\n",
    "\n",
    "# Hint: use the len() function and practice list comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For loop\n",
    "clean_email_lengths = []\n",
    "for sublist in stemmed_nested_list:\n",
    "    clean_email_lengths.append(len(sublist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python List Comprehension\n",
    "clean_email_lengths = [len(sublist) for sublist in stemmed_nested_list]\n",
    "print('Nr words in the longest email:', max(clean_email_lengths))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Email position in the list (and the data dataframe)', np.argmax(clean_email_lengths))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_nested_list[np.argmax(clean_email_lengths)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.at[np.argmax(clean_email_lengths), 'MESSAGE']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Features & a Sparse Matrix\n",
    "\n",
    "### Creating a DataFrame with one Word per Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(stemmed_nested_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(stemmed_nested_list.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_columns_df = pd.DataFrame.from_records(stemmed_nested_list.tolist())\n",
    "word_columns_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_columns_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the Data into a Training and Testing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge: Can you split the data into a training and testing set? Set the test size at 30%. \n",
    "# The training data should include 4057 emails. Use a seed value of 42 to shuffle the data. \n",
    "# What should the target values be? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(word_columns_df, data.CATEGORY,\n",
    "                                                   test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Nr of training samples', X_train.shape[0])\n",
    "print('Fraction of training set', X_train.shape[0] / word_columns_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.index.name = X_test.index.name = 'DOC_ID'\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Create a Sparse Matrix for the Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = pd.Index(vocab.VOCAB_WORD)\n",
    "type(word_index[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index.get_loc('thu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sparse_matrix(df, indexed_words, labels):\n",
    "    \"\"\"\n",
    "    Returns sparse matrix as dataframe.\n",
    "    \n",
    "    df: A dataframe with words in the columns with a document id as an index (X_train or X_test)\n",
    "    indexed_words: index of words ordered by word id\n",
    "    labels: category as a series (y_train or y_test)\n",
    "    \"\"\"\n",
    "    \n",
    "    nr_rows = df.shape[0]\n",
    "    nr_cols = df.shape[1]\n",
    "    word_set = set(indexed_words)\n",
    "    dict_list = []\n",
    "    \n",
    "    for i in range(nr_rows):\n",
    "        for j in range(nr_cols):\n",
    "            \n",
    "            word = df.iat[i, j]\n",
    "            if word in word_set:\n",
    "                doc_id = df.index[i]\n",
    "                word_id = indexed_words.get_loc(word)\n",
    "                category = labels.at[doc_id]\n",
    "                \n",
    "                item = {'LABEL': category, 'DOC_ID': doc_id,\n",
    "                       'OCCURENCE': 1, 'WORD_ID': word_id}\n",
    "                \n",
    "                dict_list.append(item)\n",
    "    \n",
    "    return pd.DataFrame(dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sparse_train_df = make_sparse_matrix(X_train, word_index, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_train_df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_train_df[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Combine Occurrences with the Pandas groupby() Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_grouped = sparse_train_df.groupby(['DOC_ID', 'WORD_ID', 'LABEL']).sum()\n",
    "train_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.at[0, 'VOCAB_WORD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.MESSAGE[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_grouped = train_grouped.reset_index()\n",
    "train_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_grouped.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.at[1923, 'VOCAB_WORD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.MESSAGE[5795]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_grouped.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Training Data as .txt File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(TRAINING_DATA_FILE, train_grouped, fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_grouped.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge\n",
    "\n",
    "Can you create a sparse matrix for the test data. Group the occurrences of the same word in the same email. Then save the data as a .txt file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sparse_test_df = make_sparse_matrix(X_test, word_index, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_grouped = sparse_test_df.groupby(['DOC_ID', 'WORD_ID', 'LABEL']).sum().reset_index()\n",
    "test_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_grouped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(TEST_DATA_FILE, test_grouped, fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Pre-Processing Subtleties and Checking your Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Challenge: We started with 5796 emails. We split it into 4057 emails for training and 1739 emails for testing. \n",
    "\n",
    "How many individual emails were included in the testing .txt file? Count the number in the test_grouped DataFrame. After splitting and shuffling our data, how many emails were included in the X_test DataFrame? Is the number the same? If not, which emails were excluded and why? Compare the DOC_ID values to find out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc_ids = set(train_grouped.DOC_ID)\n",
    "test_doc_ids = set(test_grouped.DOC_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_doc_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(X_test.index.values) - test_doc_ids # Excluded emails after pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.MESSAGE[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_msg_no_html(data.at[14, 'MESSAGE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.MESSAGE[1096]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_msg_no_html(data.at[1096, 'MESSAGE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_message(data.at[1096, 'MESSAGE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
